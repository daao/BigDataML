{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence model with Kafka and Spark streaming \n",
    "\n",
    "This notebook provides an example of a persistent model on streaming data coming from a Kafka producer. \n",
    "\n",
    "This notebook uses \n",
    "* the [Python client for the Apache Kafka distributed stream processing system](http://kafka-python.readthedocs.io/en/master/index.html) to receive messages from a Kafka cluster. \n",
    "* [Spark streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html) for processing the streaming data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re, ast\n",
    "import numpy as np\n",
    "import os\n",
    "import padasip as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars spark-streaming-kafka-0-8-assembly_2.11-2.2.1.jar ' +\\\n",
    "                                '--conf spark.driver.memory=2g  pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"KafkaReceiveMB\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Kafka server on topic persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a connection to a Kafka stream\n",
    "#You may change the topic, or batch interval\n",
    "#The Zookeeper server is assumed to be running at 127.0.0.1:2181\n",
    "#The function returns the Spark context, Spark streaming context, and DStream object\n",
    "def getKafkaDStream(spark,topic='persistence',batch_interval=10):\n",
    "\n",
    "    #Get Spark context\n",
    "    sc=spark.sparkContext\n",
    "\n",
    "    #Create streaming context, with required batch interval\n",
    "    ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "    #Checkpointing needed for stateful transforms\n",
    "    ssc.checkpoint(\"checkpoint\")\n",
    "    \n",
    "    #Create a DStream that represents streaming data from Kafka, for the required topic \n",
    "    dstream = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {topic: 1})\n",
    "    \n",
    "    return [sc,ssc,dstream]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for the Exponientally Weighted Average\n",
    "\n",
    "$ v_t = \\beta \\cdot v_{t-1} + (1-\\beta) \\cdot \\theta_t $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $v_t$ is the prediction, $\\beta$ is the probability and $\\theta_t$ is observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentially_weighted_average(v, theta):\n",
    "    return beta*v + (1-beta)*theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update function: new_values are the set of values received during the batch interval, state is the current state\n",
    "#The state is assumed to be a list of two values: the last temperature, and output data for day 8 (predictions, truth, seconds)\n",
    "#The function estimates the prediction error on the set of new values, and update the state for the persistence model\n",
    "def update_ewa(new_values, state): \n",
    "\n",
    "    last_temperature=state[0]\n",
    "    sensorToPredict=state[1]\n",
    "    output_day8=state[2]\n",
    "    pred_mod = state[3]\n",
    "\n",
    "    if len(new_values)>0 :\n",
    "        #Transforms list of values into an array\n",
    "        array_values=np.array(new_values)\n",
    "        \n",
    "        #if day 8\n",
    "        if np.floor(array_values[0][1] / 86400)==7:\n",
    "            \n",
    "            predictions=[]\n",
    "            truth=[]\n",
    "            seconds=[]\n",
    "            \n",
    "            v = 0\n",
    "            #Go through all measurements\n",
    "            for i in range(0,array_values.shape[0]):\n",
    "                \n",
    "                if array_values[i,2]==sensorToPredict:                    \n",
    "                    # Get the the last temperature saw or the last possible temperature\n",
    "                    t = last_temperature[:i][-1] if i >= len(last_temperature) else last_temperature[i]\n",
    "                    \n",
    "                    # Compute the exponentially weighted average\n",
    "                    # vt = B*v(t-1) + (1-B)*t\n",
    "                    # where vt is the prediction, B is a probability and t is the last temperature shown\n",
    "                    prediction = exponentially_weighted_average(v, t)\n",
    "                    \n",
    "                    # Append the prediction\n",
    "                    predictions.append(prediction)\n",
    "                    \n",
    "                    # Set the v for the next computation of EWA\n",
    "                    v = prediction\n",
    "                    \n",
    "                    truth.append(array_values[i,0])\n",
    "                    seconds.append(array_values[i,1])\n",
    "                    \n",
    "            #Store data in state\n",
    "            output_day8=[predictions,truth,seconds]\n",
    "            \n",
    "        else:\n",
    "            if array_values[0][1] % 86400<8:\n",
    "                #Before day 8, adapt your model with measurements of the current batch\n",
    "                # Store the last temperature for each measures\n",
    "                \n",
    "                # initialize the last temperatures array to 0\n",
    "                last_temperature = [0 for i in range(0,array_values.shape[0])]\n",
    "                for i in range(0,array_values.shape[0]):\n",
    "                    \n",
    "                    # If the sensor is one who whant to predict\n",
    "                    if array_values[i, 2]==sensorToPredict:\n",
    "                        last_temperature[i] = np.float(array_values[i,0])\n",
    "        \n",
    "    #Update state\n",
    "    state=[last_temperature, sensorToPredict,output_day8, pred_mod]\n",
    "        \n",
    "    #state is now the last received measurement\n",
    "    return (state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_rls(new_values, state):\n",
    "    filt=state[0]\n",
    "    sensorToPredict=state[1]\n",
    "    output_day8=state[2]\n",
    "    pred_mod=state[3]\n",
    "    \n",
    "    # Code for SVM prediction model process here\n",
    "    #if day 8\n",
    "    if np.floor(array_values[0][1] / 86400)==7:\n",
    "\n",
    "        predictions=[]\n",
    "        truth=[]\n",
    "        seconds=[]\n",
    "\n",
    "        v = 0\n",
    "        #Go through all measurements\n",
    "        for i in range(0,array_values.shape[0]):\n",
    "\n",
    "            if array_values[i,2]==sensorToPredict:                  \n",
    "\n",
    "                # Predict the value\n",
    "                prediction = filt.predict(i)\n",
    "\n",
    "                # Append the prediction\n",
    "                predictions.append(prediction)\n",
    "\n",
    "                # Set the v for the next computation of EWA\n",
    "                v = prediction\n",
    "\n",
    "                truth.append(array_values[i,0])\n",
    "                seconds.append(array_values[i,1])\n",
    "\n",
    "        #Store data in state\n",
    "        output_day8=[predictions,truth,seconds]\n",
    "\n",
    "    else:\n",
    "        if array_values[0][1] % 86400<8:\n",
    "            for i in range(0,array_values.shape[0]):\n",
    "\n",
    "                # If the sensor is one who whant to predict\n",
    "                if array_values[i, 2]==sensorToPredict:\n",
    "                    filt.adapt(array_values[i,0], array_values[i,1])\n",
    "\n",
    "    #Update state\n",
    "    state=[filt, sensorToPredict,output_day8, pred_mod]\n",
    "        \n",
    "    #state is now the last received measurement\n",
    "    return (state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update function: new_values are the set of values received during the batch interval, state is the current state\n",
    "#The state is assumed to be a list of two values: the last temperature, and output data for day 8 (predictions, truth, seconds)\n",
    "#The function estimates the prediction error on the set of new values, and update the state for the corresponding prediction model\n",
    "def updateFunction(new_values, state): \n",
    "    pred_mod=state[3]\n",
    "    if pred_mod == \"EWA\":\n",
    "        return update_ewa(new_values, state)\n",
    "    else:\n",
    "        return update_rls(new_values, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define streaming pipeline\n",
    "\n",
    "* We define one state, which is a list of two elements:\n",
    "    * The last measurement\n",
    "    * The output of predictions for sensor 1 for day 8\n",
    "* We create a DStream, flat map with the sensor ID as key, update state for the stream, and save MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#Print number of partitions and number of records for an RDD\n",
    "def printInfoRDD(rdd):\n",
    "    if rdd is not None:\n",
    "        print(\"The RDD has \"+str(rdd.getNumPartitions())+\" partitions\")\n",
    "        print(\"The RDD has \"+str(rdd.count())+\" elements\")\n",
    "    else:\n",
    "        print(\"No info to provide\")\n",
    "        \n",
    "#Save state in global Python variable\n",
    "def saveState(rdd):\n",
    "    global state_global\n",
    "    if rdd is not None:\n",
    "        data=rdd.collect()\n",
    "        state_global=data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial state for EWA\n",
    "last_measurement=None\n",
    "sensorToPredict=1\n",
    "output_day8=None\n",
    "pred_mod = \"EWA\"\n",
    "\n",
    "state1=[last_measurement, sensorToPredict, output_day8, pred_mod]\n",
    "\n",
    "#Crée la state que tu as besoin pour SVM (sur le capteur 24 cf énoncé)\n",
    "#Initial state for LRS\n",
    "filt = pa.filters.FilterRLS(n=1, mu=0.98)\n",
    "sensorToPredict_2=24\n",
    "output_day8_2=None\n",
    "pred_mod_2 = \"LRS\"\n",
    "state2=[filt, sensorToPredict_2, output_day8_2, pred_mod_2]\n",
    "\n",
    "\n",
    "#Batch interval (to be synchronized with KafkaSend)\n",
    "interval=10\n",
    "\n",
    "#This variable is used to retrieve state data (through saveState function)\n",
    "state_global=None\n",
    "\n",
    "#Create dtsream\n",
    "[sc,ssc,dstream]=getKafkaDStream(spark=spark,topic='mean_neighbor2',batch_interval=interval)\n",
    "\n",
    "#Evaluate string content (a list) and cast as float value\n",
    "dstream = dstream.map(lambda x: np.array(ast.literal_eval(x[1])))\n",
    "#Use this for debugging\n",
    "#dstream.pprint()\n",
    "\n",
    "#Group by sensor id. x[2] is here the sensorId (for example '1'), and x are the sensor measurement, seconds, sensorId and sensor type)\n",
    "dstream=dstream.flatMap(lambda x: [(x[2],x)])\n",
    "dstream.foreachRDD(printInfoRDD)\n",
    "#Use this for debugging\n",
    "#dstream.pprint()\n",
    "\n",
    "#initialStateRDD = sc.parallelize([(sensorToPredict,state1)])\n",
    "\n",
    "# Par exemple pour les 2 models, sinon pour tester change juste la state de la ligne du dessus\n",
    "# A la fin du projet, faut qu'on puisse lancer les 2 modeles (EWA sur capteur 1, SVM sur capteur 24)\n",
    "# et print les 2 plots\n",
    "initialStateRDD = sc.parallelize([(sensorToPredict,state1), (sensorToPredict_2, state2)])\n",
    "print(\"Number of partitions for StateRDD: \"+str(initialStateRDD.getNumPartitions()))\n",
    "\n",
    "state_stream=dstream.updateStateByKey(updateFunction,initialRDD=initialStateRDD)\n",
    "      \n",
    "state_stream.foreachRDD(printInfoRDD)\n",
    "state_stream.foreachRDD(saveState)\n",
    "#Use this for debugging\n",
    "#dstream.pprint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start streaming application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For synchronization with receiver (for the sake of the simulation), starts at a number of seconds multiple of five\n",
    "current_time=time.time()\n",
    "time_to_wait=interval-current_time%interval\n",
    "time.sleep(time_to_wait)\n",
    "\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wait to receive all data up to day 8 before stopping\n",
    "ssc.stop(stopSparkContext=False,stopGraceFully=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect results, plot predictions, compute MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For plots\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = EWA, 1 = RLS\n",
    "state_global[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[predictions,truth,seconds]=state_global[0][1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE=np.mean((np.array(truth)-np.array(predictions))**2)\n",
    "print(\"MSE of model on day 8: \"+str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_truth = go.Scatter(\n",
    "    y = truth,\n",
    "    x = seconds,\n",
    "    name=\"Truth\"\n",
    ")\n",
    "\n",
    "trace_predictions = go.Scatter(\n",
    "    y = predictions,\n",
    "    x = seconds,\n",
    "    name=\"Predictions\"\n",
    ")\n",
    "\n",
    "layout= go.Layout(\n",
    "    title= 'Truth and predictions for sensor 1, day 8<br>Persistence model <br>'+\\\n",
    "            'Mean square error: '+str(MSE),\n",
    "    xaxis= dict(\n",
    "        title= 'Time (seconds)',\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Temperature',\n",
    "    ),\n",
    "    showlegend= True\n",
    ")\n",
    "\n",
    "fig= go.Figure(data=[trace_truth,trace_predictions], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
